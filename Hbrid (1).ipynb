{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dba3GJJtWd6T",
        "outputId": "3d399d7a-469d-4d2d-d538-d9411174c81c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gloss: sloping land (especially the slope beside a body of water)\n",
            "context: {'the', 'they', 'body', 'on', 'canoe', 'currents', 'sloping', ')', 'river', 'slope', 'up', 'bank', 'pulled', '(', 'especially', 'of', 'he', 'water', 'and', 'a', 'land', 'sat', 'beside', 'watched'}\n",
            "gloss: a financial institution that accepts deposits and channels the money into lending activities\n",
            "context: {'at', 'holds', 'the', 'that', 'into', 'check', 'on', 'my', 'financial', 'bank', 'money', 'channels', 'mortgage', 'institution', 'cashed', 'lending', 'he', 'deposits', 'a', 'and', 'activities', 'home', 'accepts'}\n",
            "gloss: a long ridge or pile\n",
            "context: {'or', 'pile', 'of', 'earth', 'long', 'ridge', 'a', 'huge', 'bank'}\n",
            "gloss: an arrangement of similar objects in a row or in tiers\n",
            "context: {'similar', 'or', 'tiers', 'of', 'arrangement', 'an', 'in', 'he', 'row', 'a', 'objects', 'operated', 'switches', 'bank'}\n",
            "gloss: a supply or stock held in reserve for future use (especially in emergencies)\n",
            "context: {'for', 'or', 'use', 'held', 'reserve', 'especially', 'in', 'supply', 'stock', 'emergencies', 'a', ')', 'future', 'bank', '('}\n",
            "gloss: the funds held by a gambling house or the dealer in some gambling games\n",
            "context: {'at', 'the', 'or', 'held', 'monte', 'games', 'break', 'tried', 'some', 'funds', 'bank', 'to', 'dealer', 'in', 'gambling', 'he', 'a', 'house', 'carlo', 'by'}\n",
            "gloss: a slope in the turn of a road or track; the outside is higher than the inside in order to reduce the effects of centrifugal force\n",
            "context: {'inside', 'the', 'or', 'force', 'outside', 'effects', 'track', 'order', 'turn', 'road', 'slope', 'bank', ';', 'is', 'of', 'in', 'than', 'a', 'higher', 'to', 'reduce', 'centrifugal'}\n",
            "gloss: a container (usually with a slot in the top) for keeping money at home\n",
            "context: {'slot', 'for', 'the', 'at', 'coin', 'with', 'empty', 'usually', 'keeping', 'container', ')', 'bank', 'money', '(', 'top', 'in', 'a', 'home', 'was'}\n",
            "gloss: a building in which the business of banking transacted\n",
            "context: {'the', 'business', 'banking', 'on', 'bank', 'is', 'of', 'building', 'in', 'transacted', 'nassau', 'and', 'a', 'corner', 'which', 'witherspoon'}\n",
            "gloss: a flight maneuver; aircraft tips laterally about its longitudinal axis (especially in turning)\n",
            "context: {'the', 'laterally', 'into', 'steep', 'plane', 'longitudinal', ')', 'its', 'turning', ';', '(', 'flight', 'bank', 'tips', 'especially', 'axis', 'in', 'went', 'a', 'aircraft', 'maneuver', 'about'}\n",
            "gloss: tip laterally\n",
            "context: {'the', 'tip', 'laterally', 'pilot', 'aircraft', 'to', 'had', 'bank'}\n",
            "gloss: enclose with a bank\n",
            "context: {'a', 'roads', 'with', 'enclose', 'bank'}\n",
            "gloss: do business with a bank or keep an account at a bank\n",
            "context: {'at', 'do', 'business', 'or', 'with', 'this', 'you', 'account', '?', 'bank', 'keep', 'an', 'in', 'town', 'a', 'where'}\n",
            "gloss: act as the banker in a game or in gambling\n",
            "context: {'the', 'or', 'game', 'in', 'gambling', 'as', 'banker', 'a', 'bank', 'act'}\n",
            "gloss: be in the banking business\n",
            "context: {'the', 'business', 'bank', 'banking', 'in', 'be'}\n",
            "gloss: put into a bank account\n",
            "context: {'every', 'account', 'she', 'month', 'into', 'deposits', 'a', 'her', 'paycheck', 'put', 'bank'}\n",
            "gloss: cover with ashes so to control the rate of burning\n",
            "context: {'cover', 'so', 'the', 'fire', 'ashes', 'with', 'of', 'rate', 'a', 'burning', 'to', 'control', 'bank'}\n",
            "gloss: have confidence or faith in\n",
            "context: {'or', 'trust', 'can', 'i', 'good', 'on', 'my', 'recipes', 'god', 'bank', 'grandmother', 'have', 'swear', \"'s\", 'we', 'education', 'in', 'friends', 'rely', 'faith', 'your', 'confidence', 'by'}\n",
            "Word: bank\n",
            "Sense: a financial institution that accepts deposits and channels the money into lending activities\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "\n",
        "\n",
        "def lesk_algorithm(word, sentence):\n",
        "    best_sense = None\n",
        "    max_overlap = 0\n",
        "    word_synsets = wordnet.synsets(word)\n",
        "    #print(\"word_synset:\",word_synsets)\n",
        "\n",
        "    tokens = word_tokenize(sentence)\n",
        "    #print(\"word sysets:\",tokens)\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "    #print(\"pos_tags:\", pos_tags)\n",
        "    target_word_pos = None\n",
        "\n",
        "    for i, (token, pos) in enumerate(pos_tags):\n",
        "        if token.lower() == word.lower():\n",
        "            target_word_pos = pos\n",
        "            break\n",
        "    if target_word_pos is not None and target_word_pos.startswith('N'):\n",
        "       context_words = set()\n",
        "       window_size = 10\n",
        "       for j in range(i - window_size, i + window_size + 1):\n",
        "         if 0 <= j < len(pos_tags) and pos_tags[j][1].startswith('N') :  # Consider only nouns for context\n",
        "            context_words.add(pos_tags[j][0])\n",
        "\n",
        "\n",
        "       for synset in word_synsets:\n",
        "         gloss = synset.definition()\n",
        "         print(\"gloss:\", gloss)\n",
        "         examples = synset.examples()\n",
        "         context = set(word_tokenize(gloss.lower())) | set(word_tokenize(' '.join(examples).lower())) | {word}\n",
        "         print(\"context:\",context)\n",
        "         overlap = len(context_words.intersection(context))\n",
        "         if overlap > max_overlap:\n",
        "            max_overlap = overlap\n",
        "            best_sense = synset\n",
        "       return best_sense\n",
        "\n",
        "    context_words = set(word_tokenize(sentence))\n",
        "\n",
        "    for synset in word_synsets:\n",
        "        gloss = synset.definition()\n",
        "        examples = synset.examples()\n",
        "        context = set(word_tokenize(gloss)) | set(word_tokenize(' '.join(examples)))\n",
        "\n",
        "        overlap = len(context_words.intersection(context))\n",
        "\n",
        "        if overlap > max_overlap:\n",
        "            max_overlap = overlap\n",
        "            best_sense = synset\n",
        "\n",
        "    return best_sense\n",
        "\n",
        "# Example usage\n",
        "word = \"bank\"\n",
        "sentence =\"I deposite money to bank \"\n",
        "sense = lesk_algorithm(word, sentence)\n",
        "if sense:\n",
        "    print(\"Word:\", word)\n",
        "    print(\"Sense:\", sense.definition())\n",
        "else:\n",
        "    print(\"No sense found for the given word.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "# Download WordNet data if not already downloaded\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def get_wordnet_definition(lemma, pos, sense_num):\n",
        "    synset_id = f\"{lemma}.{pos}.{sense_num}\"\n",
        "    try:\n",
        "        synset = wn.synset(synset_id)\n",
        "        return synset.definition()\n",
        "    except Exception as e:\n",
        "        print(f\"Error retrieving definition for {synset_id}: {e}\")\n",
        "        return \"Definition not found.\"\n",
        "\n",
        "def predict_sense_with_definition(model, sentence, target_word):\n",
        "    # Tokenize and index words\n",
        "    sentence = sentence.split()\n",
        "    sentence_idx = [preProcessDataset.word2idx.get(word.lower(), preProcessDataset.word2idx['<unk>']) for word in sentence]\n",
        "\n",
        "    # Pad the sequence\n",
        "    while len(sentence_idx) < preProcessDataset.max_len:\n",
        "        sentence_idx.append(preProcessDataset.word2idx['<pad>'])\n",
        "\n",
        "    # Convert target word to index\n",
        "    target_word_idx = preProcessDataset.target2idx.get(target_word.lower(), preProcessDataset.target2idx['<unk>'])\n",
        "\n",
        "    # Convert to Torch Tensor\n",
        "    input_tensor = torch.tensor([sentence_idx]).cuda()\n",
        "    target_word_tensor = torch.tensor([target_word_idx]).cuda()\n",
        "\n",
        "    # Forward Pass\n",
        "    output = model(input_tensor, target_word_tensor)\n",
        "\n",
        "    # Interpret Output\n",
        "    predicted_sense_idx = torch.argmax(output, dim=1).item()\n",
        "    predicted_sense = preProcessDataset.idx2sense[predicted_sense_idx]\n",
        "\n",
        "    # Extract lemma, pos, and sense number from the predicted sense\n",
        "    sense_components = predicted_sense.split('%')\n",
        "    if len(sense_components) >= 3:\n",
        "        lemma, pos, sense_num = sense_components[:3]\n",
        "    else:\n",
        "        lemma = sense_components[0]\n",
        "        pos = \"n\"  # Default to noun if part of speech is not available\n",
        "        sense_num = \"01\"  # Default sense number\n",
        "\n",
        "    # Get WordNet definition\n",
        "    definition = get_wordnet_definition(lemma, pos, sense_num)\n",
        "\n",
        "    return predicted_sense, definition\n"
      ],
      "metadata": {
        "id": "dr8dl8Ltl2B5",
        "outputId": "f7ebc027-3740-4d97-eaaf-ce96aa2fa3d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZfihL4wWovs",
        "outputId": "550fc536-40f2-4dc6-87db-6412ce20450c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8018/8018 [00:43<00:00, 184.56it/s]\n",
            "100%|██████████| 2005/2005 [00:06<00:00, 311.30it/s]\n",
            "100%|██████████| 8112/8112 [00:01<00:00, 5957.61it/s]\n",
            "100%|██████████| 8112/8112 [00:01<00:00, 7933.58it/s]\n",
            "100%|██████████| 2029/2029 [00:00<00:00, 7728.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "biLSTMModel(\n",
            "  (embedding): Embedding(4736, 300)\n",
            "  (lstm): LSTM(300, 128, bidirectional=True)\n",
            "  (linear): Linear(in_features=256, out_features=4157, bias=True)\n",
            ")\n",
            "Epoch : 1/2 | Loss : 0.7630 | Accuracy : 0.7425\n",
            "Epoch : 1/2 | Validation Loss : 0.5725 | Validation Accuracy : 0.7246\n",
            "Epoch : 2/2 | Loss : 0.4765 | Accuracy : 0.8063\n",
            "Epoch : 2/2 | Validation Loss : 0.5734 | Validation Accuracy : 0.7215\n",
            "| Testing Loss : 0.5277 | Testing Accuracy : 0.7610\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Error retrieving definition for <unk>.n.01: No lemma '<unk>' with part of speech 'n'\n",
            "Predicted Sense: <unk>\n",
            "Definition: Definition not found.\n"
          ]
        }
      ],
      "source": [
        "#pip install -U scikit-learn pandas torch\n",
        "import nltk\n",
        "from nltk.corpus import semcor\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "# from transformers import BertTokenizer\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"semcor_copy.csv\")\n",
        "#df\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_data, test_data = train_test_split(df, test_size=0.2)\n",
        "from tqdm import tqdm\n",
        "def getNewData(data):\n",
        "    new_data = pd.DataFrame(columns=['sentence','target_word', 'sense', 'gloss'])\n",
        "\n",
        "    for i in tqdm(range(0,len(data))):\n",
        "        sentence = data.iloc[i]['sentence']\n",
        "        idx1 = sentence.find('[TGT]')\n",
        "        idx2 = sentence.find('[TGT]', idx1+1)\n",
        "        target_word = sentence[idx1+6:idx2-1]\n",
        "        sentence = sentence.replace('[TGT]', '')\n",
        "        sense_keys = data.iloc[i]['sense_keys']\n",
        "        glosses = data.iloc[i]['glosses']\n",
        "        target = data.iloc[i]['target']\n",
        "        sense_keys = sense_keys.strip('[]')\n",
        "        sense_keys = sense_keys.split(',')\n",
        "        target = target.strip('[]')\n",
        "        target = target.split(',')\n",
        "        glosses = glosses.strip('[]')\n",
        "        glosses = glosses.split(',')\n",
        "        # for every target value add the correspodinign sense key in a new column and also a new column for the gloss\n",
        "        for j in range(0,len(target)):\n",
        "            tgt = int(target[j])\n",
        "            new_row = {'sentence': sentence, 'sense': sense_keys[tgt], 'gloss': glosses[tgt], 'target_word': target_word}\n",
        "            new_data = pd.concat([new_data, pd.DataFrame(new_row, index=[0])], ignore_index=True)\n",
        "            new_data['sense'] = new_data['sense'].str.replace('\"', '')\n",
        "            new_data['sense'] = new_data['sense'].str.replace(\"'\", '')\n",
        "    return new_data\n",
        "\n",
        "train_data = getNewData(train_data[:30000])\n",
        "test_data = getNewData(test_data)\n",
        "train_data.head(10)\n",
        "target_word_idx = {}\n",
        "idx_to_target = {}\n",
        "sense_labels = []\n",
        "lemma_2_sense = {}\n",
        "for i in range(0,len(train_data)):\n",
        "    sense_label = train_data.iloc[i]['sense']\n",
        "    sense_label = sense_label.replace(' ','')\n",
        "    lemma, pos, wnsn,wnsn2 = sense_label.split('%')[0], int(sense_label.split(\n",
        "        '%')[1].split(':')[0]), sense_label.split('%')[1].split(':')[1],sense_label.split('%')[1].split(':')[2]\n",
        "    new_label = lemma + '%' + str(pos) + '%' + wnsn + '%' + wnsn2\n",
        "    if lemma not in lemma_2_sense:\n",
        "        lemma_2_sense[lemma] = []\n",
        "        target_word_idx[lemma] = len(target_word_idx)\n",
        "        idx_to_target[len(idx_to_target)] = lemma\n",
        "    if sense_label not in lemma_2_sense[lemma]:\n",
        "        lemma_2_sense[lemma].append(sense_label)\n",
        "    # sense_labels.append(new_label)\n",
        "target_word_idx['<unk>'] = len(target_word_idx)\n",
        "idx_to_target[len(idx_to_target)] = '<unk>'\n",
        "lemma_2_sense['<unk>'] = ['<unk>']\n",
        "lemma_2_sense['long']\n",
        "new_data = pd.read_csv('semcor_lstm.csv')\n",
        "new_data\n",
        "# find the max length of the sentence\n",
        "\n",
        "class preProcessDataset():\n",
        "    def __init__(self,data,min_freq):\n",
        "        self.data = data\n",
        "        self.min_freq = min_freq\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.vocab = []\n",
        "        self.vocab_sense = []\n",
        "        self.sense2idx = {}\n",
        "        self.idx2sense = {}\n",
        "        self.max_len = 0\n",
        "        self.wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "        self.lemma_2_sense = {}\n",
        "        self.wordFreq = {}\n",
        "        self.target2idx = target_word_idx\n",
        "        self.word2idx['<pad>'] = len(self.word2idx)\n",
        "        self.idx2word[len(self.idx2word)] = '<pad>'\n",
        "        self.vocab.append('<pad>')\n",
        "        self.vocab.append('<unk>')\n",
        "        self.word2idx['<unk>'] = len(self.word2idx)\n",
        "        self.idx2word[len(self.idx2word)] = '<unk>'\n",
        "        self.vocab_sense.append('<unk>')\n",
        "        self.sense2idx['<unk>'] = len(self.sense2idx)\n",
        "        self.idx2sense[len(self.idx2sense)] = '<unk>'\n",
        "\n",
        "\n",
        "\n",
        "        data = self.data\n",
        "        for i in tqdm(range(len(data))):\n",
        "            sentence = data.iloc[i]['sentence']\n",
        "            target_word = data.iloc[i]['sense'].split('%')[0]\n",
        "            target_word = target_word.lower()\n",
        "            target_word = target_word.replace(' ','')\n",
        "            sense_keys = data.iloc[i]['sense']\n",
        "            sense_keys = sense_keys.replace(' ','')\n",
        "\n",
        "            sentence = sentence.split()\n",
        "            # count freq of words\n",
        "            for word in sentence:\n",
        "                word = word.lower()\n",
        "                if word not in self.wordFreq:\n",
        "                    self.wordFreq[word] = 0\n",
        "                self.wordFreq[word] += 1\n",
        "\n",
        "\n",
        "            for word in sentence:\n",
        "                word = word.lower()\n",
        "                # punctuation marks\n",
        "                if word in ['.',',','?','!',';',':','(',')','[',']','{','}',\"'\",'\"']:\n",
        "                    continue\n",
        "                if self.wordFreq[word] < self.min_freq:\n",
        "                    word = '<unk>'\n",
        "                if word not in self.word2idx:\n",
        "                    self.word2idx[word] = len(self.word2idx)\n",
        "                    self.idx2word[len(self.idx2word)] = word\n",
        "                    self.vocab.append(word)\n",
        "            if len(sentence) > self.max_len:\n",
        "                self.max_len = len(sentence)\n",
        "            if sense_keys not in self.sense2idx:\n",
        "                self.sense2idx[sense_keys] = len(self.sense2idx)\n",
        "                self.idx2sense[len(self.idx2sense)] = sense_keys\n",
        "                self.vocab_sense.append(sense_keys)\n",
        "\n",
        "\n",
        "class getDataset(Dataset):\n",
        "    def __init__(self, data, word2idx, sense2idx, max_len, target2idx,idx2word,wordFreq,vocab):\n",
        "        self.data = data\n",
        "        self.word2idx = word2idx\n",
        "        self.sense2idx = sense2idx\n",
        "        self.max_len = max_len\n",
        "        self.target2idx = target2idx\n",
        "        self.idx2word = idx2word\n",
        "        self.wordFreq = wordFreq\n",
        "        self.vocab = vocab\n",
        "        self.input_data = []\n",
        "        self.sense_data = []\n",
        "        self.target2word = []\n",
        "\n",
        "\n",
        "        for i in tqdm(range(len(data))):\n",
        "            sentence = data.iloc[i]['sentence']\n",
        "            sense_keys = data.iloc[i]['sense']\n",
        "            sense_keys = sense_keys.replace(' ','')\n",
        "            target_word = sense_keys.split('%')[0]\n",
        "            target_word = target_word.lower()\n",
        "\n",
        "            target_word = target_word.replace(' ','')\n",
        "            sense_keys = sense_keys.replace(' ','')\n",
        "            sentence = sentence.split()\n",
        "            sentence_idx = []\n",
        "            sense_idx = []\n",
        "            for word in sentence:\n",
        "                word = word.lower()\n",
        "                # punctuation marks\n",
        "                if word in ['.',',','?','!',';',':','(',')','[',']','{','}',\"'\",'\"']:\n",
        "                    continue\n",
        "                if word not in self.word2idx:\n",
        "                    word = '<unk>'\n",
        "                sentence_idx.append(self.word2idx[word])\n",
        "            while len(sentence_idx) < self.max_len:\n",
        "                sentence_idx.append(self.word2idx['<pad>'])\n",
        "            self.input_data.append(sentence_idx)\n",
        "            # sense_idx.append(self.sense2idx[sense_keys])\n",
        "            if sense_keys not in self.sense2idx:\n",
        "                sense_keys = '<unk>'\n",
        "            self.sense_data.append(self.sense2idx[sense_keys])\n",
        "            if target_word not in self.target2idx:\n",
        "                target_word = '<unk>'\n",
        "            self.target2word.append(self.target2idx[target_word])\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        return torch.tensor(self.input_data[idx]),torch.tensor(self.sense_data[idx]),torch.tensor(self.target2word[idx])\n",
        "\n",
        "\n",
        "preProcessDataset = preProcessDataset(train_data,2)\n",
        "trainData = getDataset(train_data,preProcessDataset.word2idx,preProcessDataset.sense2idx,preProcessDataset.max_len,preProcessDataset.target2idx,preProcessDataset.idx2word,preProcessDataset.wordFreq,preProcessDataset.vocab)\n",
        "testData = getDataset(test_data, preProcessDataset.word2idx, preProcessDataset.sense2idx, preProcessDataset.max_len,\n",
        "                      preProcessDataset.target2idx, preProcessDataset.idx2word, preProcessDataset.wordFreq, preProcessDataset.vocab)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_dataset, valid_dataset = train_test_split(trainData, test_size=0.2, random_state=42)\n",
        "# dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=8, shuffle=True)\n",
        "test_dataloader = DataLoader(testData, batch_size=8, shuffle=True)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class biLSTMModel(nn.Module):\n",
        "    def __init__(self,input_size,hidden_size,sense_vocab,embedding_dim,dataset):\n",
        "        super(biLSTMModel,self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.sense_vocab = sense_vocab\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.dataset = dataset\n",
        "        self.embedding = nn.Embedding(self.input_size,self.embedding_dim)\n",
        "        self.lstm = nn.LSTM(self.embedding_dim,self.hidden_size,bidirectional=True)\n",
        "        self.linear = nn.Linear(self.hidden_size*2,len(self.sense_vocab))\n",
        "        self.sense2idx = self.dataset.sense2idx\n",
        "        self.idx2word = idx_to_target\n",
        "\n",
        "    def forward(self,x,target_word):\n",
        "        x = self.embedding(x)\n",
        "        x = x.permute(1,0,2)\n",
        "        output,(hidden,cell) = self.lstm(x)\n",
        "        hidden = torch.cat((hidden[-2,:,:],hidden[-1,:,:]),dim=1)\n",
        "        out = self.linear(hidden)\n",
        "        for i,target_wo in enumerate(target_word):\n",
        "            target_wo = idx_to_target[target_wo.item()]\n",
        "            target_word_sense = lemma_2_sense[target_wo]\n",
        "            target_word_sense_idx = [self.sense2idx[sense] for sense in target_word_sense]\n",
        "            out[i,target_word_sense_idx] = F.softmax(out[i,target_word_sense_idx],dim=0)\n",
        "\n",
        "        return out\n",
        "\n",
        "from torch.nn import CrossEntropyLoss\n",
        "model = biLSTMModel(len(preProcessDataset.word2idx),128,preProcessDataset.vocab_sense,300,preProcessDataset)\n",
        "model = model.cuda()\n",
        "criterion = CrossEntropyLoss(ignore_index=preProcessDataset.word2idx['<pad>'])\n",
        "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
        "print(model)\n",
        "num_epochs = 2\n",
        "for epoch in range(num_epochs):\n",
        "    total_correct = 0\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "    for i,(sentence,sense,target_word) in enumerate(train_dataloader):\n",
        "        sentence = sentence.cuda()\n",
        "        sense = sense.cuda()\n",
        "        target_word = target_word.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(sentence,target_word)\n",
        "        # print(output)\n",
        "        loss = criterion(output,sense)\n",
        "        pred_sense = torch.argmax(output,dim=1)\n",
        "        correct = torch.sum(pred_sense == sense)\n",
        "        total_correct += correct.item()\n",
        "        loss.backward()\n",
        "        total_loss += loss.item()\n",
        "        optimizer.step()\n",
        "    print('Epoch : {}/{} | Loss : {:.4f} | Accuracy : {:.4f}'.format(epoch+1,num_epochs,total_loss/len(train_dataloader),total_correct/len(train_dataset)))\n",
        "\n",
        "    # validation on test dataset\n",
        "    total_correct = 0\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i,(sentence,sense,target_word) in enumerate(valid_dataloader):\n",
        "            sentence = sentence.cuda()\n",
        "            sense = sense.cuda()\n",
        "            target_word = target_word.cuda()\n",
        "            output = model(sentence,target_word)\n",
        "            loss = criterion(output,sense)\n",
        "            pred_sense = torch.argmax(output,dim=1)\n",
        "            correct = torch.sum(pred_sense == sense)\n",
        "            total_correct += correct.item()\n",
        "            total_loss += loss.item()\n",
        "        print('Epoch : {}/{} | Validation Loss : {:.4f} | Validation Accuracy : {:.4f}'.format(epoch+1,num_epochs,total_loss/len(valid_dataloader),total_correct/len(valid_dataset)))\n",
        "\n",
        "torch.save(model.state_dict(),'biLSTM_model.pth')\n",
        "\n",
        "total_correct = 0\n",
        "total_loss = 0\n",
        "with torch.no_grad():\n",
        "    for i, (sentence, sense, target_word) in enumerate(test_dataloader):\n",
        "        sentence = sentence.cuda()\n",
        "        sense = sense.cuda()\n",
        "        target_word = target_word.cuda()\n",
        "        output = model(sentence, target_word)\n",
        "        loss = criterion(output, sense)\n",
        "        pred_sense = torch.argmax(output, dim=1)\n",
        "        correct = torch.sum(pred_sense == sense)\n",
        "        total_correct += correct.item()\n",
        "        total_loss += loss.item()\n",
        "    print('| Testing Loss : {:.4f} | Testing Accuracy : {:.4f}'.format(\n",
        "        total_loss/len(test_dataloader), total_correct/len(testData)))\n",
        "! pip install nltk\n",
        "sentence = \"he can play music\"\n",
        "target_word = \"can\"\n",
        "predicted_sense, definition = predict_sense_with_definition(model, sentence, target_word)\n",
        "print(\"Predicted Sense:\", predicted_sense)\n",
        "print(\"Definition:\", definition)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}